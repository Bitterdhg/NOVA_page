<!DOCTYPE html>

<head>
    <meta charset="utf-8" />
    <!-- model viewer -->
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
    <title>Autoregressive Video Generation without Vector Quantization</title>
    <link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico">
    <meta content="Autoregressive Video Generation without Vector Quantization" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-MLDP9MKGC8');
    </script>
</head>

<body>
    <!-- title -->
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">Autoregressive Video Generation without Vector Quantization</h1>
            <div class="nerf_subheader_v2">Arxiv 2024</div>
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://scholar.google.com/citations?user=S2sbvjgAAAAJ&hl=zh-CN&oi=ao" target="_blank"
                        class="nerf_authors_v2">Haoge Deng<span class="text-span_nerf"></span></a>
                    <sup>1,4*</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?&user=qQv6YbsAAAAJ" target="_blank"
                        class="nerf_authors_v2">Ting Pan<span class="text-span_nerf"></span></a>
                    <sup>2,4*</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=46eCjHQAAAAJ&hl=zh-CN" target="_blank"
                        class="nerf_authors_v2">Haiwen Diao<span class="text-span_nerf"></span></a>
                    <sup>3,4*</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=Sz1yTZsAAAAJ&hl=zh-CN" target="_blank"
                        class="nerf_authors_v2">Zhengxiong Luo<span class="text-span_nerf"></span></a>
                    <sup>4*</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=5Ydha2EAAAAJ&hl=zh-CN" target="_blank"
                        class="nerf_authors_v2">Yufeng Cui<span class="text-span_nerf"></span></a>
                    <sup>4</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=D3nE0agAAAAJ&hl=zh-CN" target="_blank"
                        class="nerf_authors_v2">Huchuan Lu<span class="text-span_nerf"></span></a>
                    <sup>3</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&hl=en" target="_blank"
                        class="nerf_authors_v2">Shiguang Shan<span class="text-span_nerf"></span></a>
                    <sup>2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com.tw/citations?user=pQNpf7cAAAAJ&hl=zh-CN&oi=ao" target="_blank"
                        class="nerf_authors_v2">Yonggang Qi<span class="text-span_nerf"></span></a>
                    <sup>1†</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=DPz0DjYAAAAJ&hl=zh-CN" target="_blank"
                        class="nerf_authors_v2">Xinlong Wang<span class="text-span_nerf"></span></a>
                    <sup>4†‡</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>BUPT</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>2 </sup>ICT-CAS</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>DLUT</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>4 </sup>BAAI</h1>,
                </div>
                <div>
                    <span class="nerf_affiliation_v2">* Equal contribution</span>,
                    <span class="nerf_affiliation_v2">† Corresponding author</span>,
                    <span class="nerf_affiliation_v2">‡ Project leader</span>,
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/pdf/2412.14169" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <a class="btn" href="paper/2412.14169v1.pdf" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a>
                    <a class="btn" href="https://github.com/baaivision/NOVA" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-github"></i> Code </a>
                    <a class="btn btn-large btn-light" href="https://huggingface.co/spaces/BAAI/nova-d48w1024-sdxl1024"
                        role="button" target="_blank" disabled>
                        <i class="fas fa-robot"></i> T2I Demo </a>
                    <a class="btn btn-large btn-light" href="https://huggingface.co/spaces/BAAI/nova-d48w1024-osp480"
                        role="button" target="_blank" disabled>
                        <i class="fas fa-robot"></i> T2V Demo </a>
                </div>

            </div>
        </div>

    </div>

    <!-- teasers-->
    <div class="white_section_nerf  w-container">
        <div class="grid-container-1">
            <img src="assets/images/model_overview.png">
            <p><strong>NOVA</strong> is a non-quantized autoregressive model for efficient and flexible visual
                generation.
            </p>
        </div>
    </div>

    <!-- abstarct -->
    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf_h2">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                This paper presents a novel approach that enables autoregressive video generation
                with high efficiency. We propose to reformulate the video generation problem as a
                non-quantized autoregressive modeling of temporal <b><i>frame-by-frame</i></b> prediction and
                spatial <b><i>set-by-set</i></b> prediction. Unlike raster-scan prediction in prior autoregressive
                models or joint distribution modeling of fixed-length tokens in diffusion models, our
                approach maintains the causal property of GPT-style models for flexible in-context
                capabilities, while leveraging bidirectional modeling within individual frames for
                efficiency. With the proposed approach, we train a novel video autoregressive
                model without vector quantization, termed <strong>NOVA</strong>. Our results demonstrate that
                <strong>NOVA</strong> surpasses prior autoregressive video models in data efficiency, inference
                speed, visual fidelity, and video fluency, even with a much smaller model capacity,
                i.e., 0.6B parameters. <strong>NOVA</strong> also outperforms state-of-the-art image diffusion
                models in text-to-image generation tasks, with a significantly lower training cost.
                Additionally, <strong>NOVA</strong> generalizes well across extended video durations and enables
                diverse zero-shot applications in one unified model.
                <br>
            </p>
        </div>
    </div>


    <!-- Method Overview -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf_h2">Method</h2>
        <div class="grid-container-1">
            <img src="assets/images/nova_framework.png" style="width: 100%; max-width: 800px;">
            <!-- <img src="assets/images/nova_framework.png"> -->
            <p><strong>NOVA framework and the inference process.</strong> With text inputs, NOVA performs autoregressive
                generation via temporal <br> frame-by-frame prediction and spatial set-by-set prediction. Finally, we
                implement diffusion denoising in a continuous-values space.
            </p>
        </div>
        <div class="grid-container-1">
            <img src="assets/images/attention.png" style="width: 100%; max-width: 800px;">
            <!-- <img src="assets/images/attention.png"> -->
            <p><strong>Overview of our block-wise temporal and spatial generalized autoregressive attention.</strong>
                Different from per-token generation, NOVA <br> regressively predicts each frame in a casual order across
                the temporal scale, and predicts each token set in a random order across the spatial scale.
            </p>
        </div>
    </div>

    <!-- Quantitative Results -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf_h2">Quantitative Results</h2>
        <div class="grid-container-1">
            <!-- <h3 class="grey-heading_nerf_h3">Text-to-image evaluation on various benchmarks.</h3> -->
            <!-- <p><strong>Text-to-image evaluation on various benchmarks.</strong> -->
            </p>
            <img src="assets/images/t2i_result.png" style="width: 100%; max-width: 800px;">
            <p><strong>NOVA outperforms existing text-to-image models with superior performance and efficiency.</strong>
            </p>
        </div>
        <div class="grid-container-1">
            <!-- <h3 class="grey-heading_nerf_h3">Text-to-video evaluation on VBench.</h3> -->
            <!-- <p><strong>2. Text-to-video evaluation on VBench.</strong> -->
            <img src="assets/images/t2v_result.png" style="width: 100%; max-width: 800px;">
            <p><strong> NOVA rivals diffusion text-to-video models and significantly suppresses the AR
                    counterpart.</strong>
            </p>
        </div>
    </div>

    <!-- Visual Results -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf_h2">Visual Results</h2>
        <!-- t2i -->
        <div class="grid-container-1">
            <h3 class="grey-heading_nerf_h3">Text-to-image Visualization</h3>
            <img src="assets/images/t2i_img.png" style="width: 100%; max-width: 800px;">
            <figcaption>
                <p>The following text prompts guided the creation of each
                    image, from left to right:
                    <br> (1) <span style="color: red;">A digital artwork of a cat</span> styled in a whimsical
                    fashion...
                    <br>(2) A solitary <span style="color: red;">lighthouse</span> standing tall against a backdrop
                    of
                    stormy seas and dark, rolling
                    clouds.
                    <br>(3) <span style="color: red;">A vibrant bouquet of wildflowers</span> on a rustic wooden
                    table.
                    <br>(4) <span style="color: red;">A selfie of an old man</span> with a white beard. <br>
                    (5)<span style="color: red;">A serene, expansive beach</span> with no people. <br>
                    (6)<span style="color: red;">A blue apple</span> and <span style="color: red;">a green
                        cup</span>.<br>
                    (7)<span style="color: red;">A chicken</span> on the bottom of <span style="color: red;">a
                        balloon</span>.
                </p>
            </figcaption>
        </div>

        <!-- t2v -->
        <div class="white_section_nerf w-container">
            <h3 class="grey-heading_nerf">Text-to-video Visualization</h2>
                <div class="grid-container-video">
                    <!-- 第一行 -->
                    <video muted autoplay controls loop class="video-item">
                        <source src="assets/videos/A 3D model of a 1800s victorian house.mp4" type="video/mp4">
                    </video>
                    <video muted autoplay controls loop class="video-item">
                        <source src="assets/videos/a dog drinking water.mp4" type="video/mp4">
                    </video>
                    <video muted autoplay controls loop class="video-item">
                        <source src="assets/videos/A panda drinking coffee in a cafe in Paris, tilt up.mp4"
                            type="video/mp4">
                    </video>
                    <!-- 第二行 -->
                    <video muted autoplay controls loop class="video-item">
                        <source
                            src="assets/videos/A space shuttle launching into orbit, with flames and smoke billowing out from the engines.mp4"
                            type="video/mp4">
                    </video>
                    <video muted autoplay controls loop class="video-item">
                        <source src="assets/videos/cogvideox1.mp4" type="video/mp4">
                    </video>
                    <video muted autoplay controls loop class="video-item">
                        <source src="assets/videos/cogvideox4.mp4" type="video/mp4">
                    </video>
                </div>
        </div>

        <div class="grid-container-1">
            <h3 class="grey-heading_nerf_h3">Zero-shot video extrapolation (33 frames -> 65 frames)</h2>
                <div class="grid-container-video-ex">
                    <!-- 第一列 -->
                    <video muted autoplay controls loop class="video-item">
                        <source src="assets/videos/sora_sample1.mp4" type="video/mp4">
                    </video>
                    <!-- 第二列 -->
                    <video muted autoplay controls loop class="video-item">
                        <source src="assets/videos/sora_sample2_2.mp4" type="video/mp4">
                    </video>
                </div>
        </div>

        <div class="grid-container-1">
            <h3 class="grey-heading_nerf_h3">Zero-shot generalization on multiple contexts</h3>
            <!-- <p><strong>2. Text-to-video generation</strong></p> -->
            <img src="assets/images/multitask.png" style="width: 100%; max-width: 800px;">
            </p>
        </div>
    </div>


    <!-- BibTeX -->
    <div class="white_section_nerf grey_container w-container">
        <h2 class="grey-heading_nerf_h2">BibTeX</h2>
        <div class="bibtex">
            <pre><code>
        @article{deng2024nova,
            title={Autoregressive Video Generation without Vector Quantization},
            author={Deng, Haoge and Pan, Ting and Diao, Haiwen and Luo, Zhengxiong and Cui, Yufeng and Lu, Huchuan and Shan, Shiguang and Qi, Yonggang and Wang, Xinlong},
            journal={arXiv preprint arXiv:2412.14169},
            year={2024}
        }</code></pre>
        </div>
    </div>

</body>
<footer>
    This project page is inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies.</a>, © Weiyu Li.
    All rights reserved.
</footer>

</html>